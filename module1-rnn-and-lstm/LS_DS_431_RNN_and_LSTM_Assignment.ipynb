{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5777367\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "data = requests.get(url)\n",
    "text = data.text\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r'[\\n\\r\\t]')\n",
    "text = regex.sub(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# unique chars list\n",
    "chars = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z', 'H', '\\x98', '¦', 'M', 'F', 'e', 'q', 'B', '\\x9c']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek at first 10\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  1155458\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 80\n",
    "# number of chars to go to the start of a new sequence\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 80 chars long\n",
    "next_char = [] \n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \"\"\"\n",
    "    Loop 0 to the length of the text - 80. Increment by the step (5)\n",
    "    \"\"\"    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "# creates a matrix of \"falses\"\n",
    "# with a shape of len(sequences), max_len, len(chars)\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "# loop through entire text\n",
    "for i, sequence in enumerate(sequences):\n",
    "    # loop through one sequence \n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "    # encode the target characters    \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1155458, 80, 111)\n",
      "y shape: (1155458, 111)\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.shape}')\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # Lecture helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Lecture Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 2.3610 - accuracy: 0.3701\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"hey did so; to the amazement of mine eyes,  That lookâd uponât.  Here comes \"\n",
      "hey did so; to the amazement of mine eyes,  That lookâd uponât.  Here comes roây ment thand wroje thes pont? w Wherse sh-mxent fatns andogse be d'd ond fokl.    HENCANTED. The lesis! in lbey   Ne.    Inde;  Ferl heprins.                           I that ind  Ifarud, ans noll?     ENDicG. Thet yof an tis in thom Olthes an, Forvame shean th mansâs led.                     Yeellay anull      Bukt foun be sor fuvr ceasmne ve kime the noup-nour you four ont; surs laus be s\n",
      "2257/2257 [==============================] - 408s 181ms/step - loss: 2.3610 - accuracy: 0.3701\n",
      "Epoch 2/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.9770 - accuracy: 0.4418\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"give you?    MERCUTIO.  The slip sir, the slip; can you not conceive?    ROMEO. \"\n",
      "give you?    MERCUTIO.  The slip sir, the slip; can you not conceive?    ROMEO. CEmat the showryone sheis, and for be stous arnceit monseze stouls  Therueary al the ir is all thy wate      Thet go to ppaty,ners Curneas, and she thast,  Berear as aye be moste sors copceis: If it, hound âus wowe with bup decus.  For bfoile, thou for you nermadrer pardy sove the fofle is fat exly you aYzamand statted  Sreand thee dy Gides und Harde, to anth.    PTHESTES. Yo, dodelt, I mould gos\n",
      "2257/2257 [==============================] - 405s 180ms/step - loss: 1.9770 - accuracy: 0.4418\n",
      "Epoch 3/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.8608 - accuracy: 0.4676\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"  You must to Pomfret, not unto the Tower.      And, madam, there is order ta'en\"\n",
      "  You must to Pomfret, not unto the Tower.      And, madam, there is order ta'en your houghs and your gentwnofâs âstinct      Though nom and live so, be- me and poinder yel  as shathâd and  shis spoblods come the reep not evaplly ploowos.    GIMENBET. You have the rames minge! EFiting thy nond.      Bit then that I leven ther yetwer fuck  Ta hank the breve my meins ferrep.    SEMEE VECE. No, whis this  my not fraters of himps fuller.    Bethe, and Oeen will move exturg \n",
      "2257/2257 [==============================] - 408s 181ms/step - loss: 1.8608 - accuracy: 0.4676\n",
      "Epoch 4/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.7846 - accuracy: 0.4879\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"quintessence of dust? Man delights not me; no, nor woman neither,  though by you\"\n",
      "quintessence of dust? Man delights not me; no, nor woman neither,  though by you frothourblettr; and last.    Wince at Mottine worch of thicha  it preaceing of his sho, I merelf ack.    ANGOW. Whan scams and be barils, and sack,  Fox sear tiret      Of forlâd sevis a dost hir some, ay elings, &t not  puiffoling gOes and jedting, be aid the soundriances sweinercoume,      My dind lofs the rrut arâd then op,      And his mult is an ser more as eresti'd hir.      Abe mangean\n",
      "2257/2257 [==============================] - 408s 181ms/step - loss: 1.7846 - accuracy: 0.4879\n",
      "Epoch 5/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.7292 - accuracy: 0.5015\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"me!     Enter Ariel.    ARIEL.  All hail, great master! grave sir, hail! I come \"\n",
      "me!     Enter Ariel.    ARIEL.  All hail, great master! grave sir, hail! I come espioned, both good owr,      To your lever weth, for a ppay say age sake heam      To achadd thot diaviar.            DUCOSE ID.    THeo with his urverciseds anfonied and thou towerbless      oprngup-mestin'd thut foe.    FORG IIG. A pleakither. He hears.    JULE. Why, abling proths friend? But I  so king. Pordmonadop, they lash fit his  me hame would my tuss there cetserâs dandy, desaly hour a\n",
      "2257/2257 [==============================] - 409s 181ms/step - loss: 1.7292 - accuracy: 0.5015\n",
      "Epoch 6/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.6864 - accuracy: 0.5126\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"     Or sack this country with a mutiny.                   Exeunt               \"\n",
      "     Or sack this country with a mutiny.                   Exeunt                DOO HiAND CoKn, Aifles to be freablohre, QUEET MANTIALER    BUCK. Pold oftion.    PARGUS.  sir. Have you are heat nembrow dewirion torspeaven of gons of Bubly.    LOLY. You a beaisons of crosing though there  And will more I seles my, you well sporfers      Of it, gear somy Carit, apâs find him: link  I elveate of une, Costure, endone Oron one doied.                         Enter SExic. WACBER \n",
      "2257/2257 [==============================] - 409s 181ms/step - loss: 1.6864 - accuracy: 0.5126\n",
      "Epoch 7/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.6515 - accuracy: 0.5218\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"Disguise the holy strength of their command,  And underwrite in an observing kin\"\n",
      "Disguise the holy strength of their command,  And underwrite in an observing king  he lase should yet distrious as drassioy again to grangers; and the      ir be in rajuesined; I are a shear he cold unsheroust, pracy  With havn our should thyer's farra. That honour wiments  ding; sheery io, and strikes, thy surphing am bofore excurnon  Busturn so man; non show'd Nastly and is with mades here.  Alatser in the stringhem may lisstient  night shesees? O, sich my frient;      Why \n",
      "2257/2257 [==============================] - 410s 181ms/step - loss: 1.6515 - accuracy: 0.5218\n",
      "Epoch 8/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.6232 - accuracy: 0.5294\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"y where.      Run, run, Orlando; carve on every tree,      The fair, the chaste,\"\n",
      "y where.      Run, run, Orlando; carve on every tree,      The fair, the chaste, an tabous      Be; gless of the whotseiver?    [Exeunt ene-Mersora, drianquall]    SA.  O, soughing,; you Comblet mould, now my love deen have  The cousson on Pricourip._]    SVANIS.  is that couple that is dose, gaden. God, live to my lancenes, of he though.    LECWIL. Why for thy dopred my fead, vinare of Scrony,  O, the baditient; nos him the have a coddence  (fazt my hearus strobis; Dods none\n",
      "2257/2257 [==============================] - 410s 182ms/step - loss: 1.6232 - accuracy: 0.5294\n",
      "Epoch 9/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.5974 - accuracy: 0.5357\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"l rouse thee, though a thousand bark.â    At this Adonis smiles as in disdain,\"\n",
      "l rouse thee, though a thousand bark.â    At this Adonis smiles as in disdain,  Tintelt_ strack, go stomes. Gake in worce comvess;        Farewellf Torgy Kithers, fless the ielf drovio:      A constices Englands.    DAUCHUCULY. I, wish this as thy man his latters,      To griahts, or pars accome by and yould  We have caure and both in the will Hicher  That was I warm not them the Euce,      What in some touring father in?    DUKE.  I do. Hang who bet he Ham to laty, the foi\n",
      "2257/2257 [==============================] - 410s 182ms/step - loss: 1.5974 - accuracy: 0.5357\n",
      "Epoch 10/10\n",
      "2257/2257 [==============================] - ETA: 0s - loss: 1.5739 - accuracy: 0.5415\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"st        mettle enough in thee to kill care.            BENEDICK.        Sir, I\"\n",
      "st        mettle enough in thee to kill care.            BENEDICK.        Sir, I hums and set from thy your gimmite alomk was swabsheching      To cienty as strighty lay the gand effer      To Even agaw yeir off your somes.    FCALIA.  We was well fiegh all combridy; that that forth and your liny works dafesading.  Then onblet this. Fowâ this monsey Engains.  O. APacion._]    KENT BUCHIOS.  But thou you nucchount, satreaâs     GOt  Bown piedsulâs ppertantr of Lordon).  \n",
      "2257/2257 [==============================] - 411s 182ms/step - loss: 1.5739 - accuracy: 0.5415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff144139320>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4NN (Python3)",
   "language": "python",
   "name": "u4nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
